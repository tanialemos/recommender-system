{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and db connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "#import streamlit as st\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up of Spark environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/02 10:38:02 WARN Utils: Your hostname, JdsThinkPad resolves to a loopback address: 127.0.1.1; using 192.168.1.8 instead (on interface wlp58s0)\n",
      "22/12/02 10:38:02 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/02 10:38:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# import findspark\n",
    "import findspark\n",
    "# initialize findspark with spark directory\n",
    "findspark.init(\"/home/juliendesmedt/spark/\")\n",
    "# import pyspark\n",
    "import pyspark\n",
    "# create spark context\n",
    "sc = pyspark.SparkContext()\n",
    "# create spark session\n",
    "spark = pyspark.sql.SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cache memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching csv from local...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_193430/188246678.py:11: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  basic = pd.read_csv(url, sep= \"\\t\")\n"
     ]
    }
   ],
   "source": [
    "from functools import lru_cache\n",
    "import time\n",
    "\n",
    "\n",
    "url = \"../Data/title_basics.tsv\"\n",
    "\n",
    "dict{query1 : cache_memory}\n",
    "\n",
    "\n",
    "@lru_cache()\n",
    "def get_csv1(url):\n",
    "    print(\"Fetching csv from local...\")\n",
    "    basic = pd.read_csv(url, sep= \"\\t\")\n",
    "    return basic\n",
    "\n",
    "\n",
    "# Execution start time\n",
    "begin = time.time()\n",
    "url = \"../Data/title_basics.tsv\"\n",
    "get_csv1(url)\n",
    "  \n",
    "# Execution end time\n",
    "end = time.time()\n",
    "\n",
    "# Execution start time 2\n",
    "begin_cache = time.time()\n",
    "url = \"../Data/title_basics.tsv\"\n",
    "get_csv1(url)\n",
    "  \n",
    "# Execution end time\n",
    "end_cache = time.time()\n",
    "\n",
    "print('no cache :'+ str(end-begin))\n",
    "print('cache :'+ str(end_cache -begin_cache ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no cache :24.450178623199463\n",
      "cache :0.0018494129180908203\n"
     ]
    }
   ],
   "source": [
    "# Execution start time\n",
    "begin_cache = time.time()\n",
    "url = \"../Data/title_basics.tsv\"\n",
    "get_csv1(url)\n",
    "  \n",
    "# Execution end time\n",
    "end_cache = time.time()\n",
    "\n",
    "print('no cache :'+ str(end-begin))\n",
    "print('cache :'+ str(end_cache -begin_cache ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading of data set under Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#/home/juliendesmedt/Documents/becode/recomendation_imdb/Data/\n",
    "# import basic data\n",
    "title_basics = spark.read.csv(\"../Data/title_basics.tsv\", header=True, inferSchema=True, sep= \"\\t\")\n",
    "\n",
    "# import crew data\n",
    "title_crew = spark.read.csv(\"../Data/title_crew.tsv\", header=True, inferSchema=True, sep= \"\\t\")\n",
    "\n",
    "# import rating data\n",
    "title_ratings = spark.read.csv(\"../Data/title_ratings.tsv\", header=True, inferSchema=True, sep= \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+--------+\n",
      "|   tconst|averageRating|numVotes|\n",
      "+---------+-------------+--------+\n",
      "|tt0000001|          5.7|    1924|\n",
      "|tt0000002|          5.8|     259|\n",
      "|tt0000003|          6.5|    1737|\n",
      "|tt0000004|          5.6|     174|\n",
      "|tt0000005|          6.2|    2550|\n",
      "+---------+-------------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- tconst: string (nullable = true)\n",
      " |-- averageRating: double (nullable = true)\n",
      " |-- numVotes: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# inspect tale\n",
    "title_ratings.show(5)\n",
    "# inspect number of partitions of data\n",
    "title_ratings.rdd.getNumPartitions()\n",
    "# inspect data types\n",
    "title_ratings.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing of data in order to suit SQL spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read in the data with an SQL statement, the classes of all columns need to be defined manually with the `StructType` and `StructField` command. The latter has three parameters:\n",
    "- `name` of the column\n",
    "- `dataType` of the column\n",
    "- `nullable`which defines whether the column can be null: true/false\n",
    "\n",
    "Note the difference in  duration between the spark and SQL command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sql types\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the schemas\n",
    "title_basicsSchema  = StructType([StructField('tconst', StringType(), True),\n",
    "                                StructField('titleType', StringType(), True),\n",
    "                                StructField('primaryTitle', StringType(), True),\n",
    "                                StructField('originalTitle', StringType(), True),\n",
    "                                StructField('isAdult', BooleanType(), True),\n",
    "                                StructField('startYear', IntegerType(), True),\n",
    "                                StructField('endYear', IntegerType(), True),\n",
    "                                StructField('runtimeMinutes', IntegerType(), True),\n",
    "                                StructField('genres', StringType(), True)])\n",
    "\n",
    "title_crewSchema  = StructType([StructField('tconst', StringType(), True),\n",
    "                              StructField('directors', StringType(), True),\n",
    "                              StructField('writers', StringType(), True)])\n",
    "\n",
    "title_ratingsSchema  = StructType([StructField('tconst', StringType(), True),\n",
    "                              StructField('averageRating', DoubleType(), True),\n",
    "                              StructField('numVotes', IntegerType(), True)]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import basic data\n",
    "title_basics = spark.read.csv(\"../Data/title_basics.tsv\", header=True, inferSchema=True, sep= \"\\t\", schema=title_basicsSchema)\n",
    "\n",
    "# import crew data\n",
    "title_crew = spark.read.csv(\"../Data/title_crew.tsv\", header=True, inferSchema=True, sep= \"\\t\", schema=title_crewSchema)\n",
    "\n",
    "# import rating data\n",
    "title_ratings = spark.read.csv(\"../Data/title_ratings.tsv\", header=True, inferSchema=True, sep= \"\\t\",schema=title_ratingsSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+--------+\n",
      "|   tconst|averageRating|numVotes|\n",
      "+---------+-------------+--------+\n",
      "|tt0000001|          5.7|    1924|\n",
      "|tt0000002|          5.8|     259|\n",
      "|tt0000003|          6.5|    1737|\n",
      "|tt0000004|          5.6|     174|\n",
      "|tt0000005|          6.2|    2550|\n",
      "+---------+-------------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- tconst: string (nullable = true)\n",
      " |-- averageRating: double (nullable = true)\n",
      " |-- numVotes: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# inspect tale\n",
    "title_ratings.show(5)\n",
    "# inspect number of partitions of data\n",
    "title_ratings.rdd.getNumPartitions()\n",
    "# inspect data types\n",
    "title_ratings.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sql functions\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the DataFrame\n",
    "title_basics.createOrReplaceTempView(\"title_basicsSQL\")\n",
    "title_crew.createOrReplaceTempView(\"title_crewSQL\")\n",
    "title_ratings.createOrReplaceTempView(\"title_ratingsSQL\")\n",
    "\n",
    "\n",
    "df = spark.sql(\"\"\"SELECT tb.tconst, tb.primaryTitle, tb.startYear, tb.genres, tc.directors, tr.averageRating, tr.numVotes\n",
    "FROM title_basicsSQL tb\n",
    "LEFT JOIN title_crewSQL tc\n",
    "ON tb.tconst = tc.tconst\n",
    "LEFT JOIN title_ratingsSQL tr\n",
    "ON tb.tconst = tr.tconst\n",
    "WHERE tr.averageRating >= 0.8 AND tr.numVotes >= 1000 \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 30:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"genres\", F.regexp_replace(str=F.col(\"genres\"), pattern=\",\", replacement=\" \")) \\\n",
    "        .withColumn(\"genres\", F.lower(\"genres\")) \\\n",
    "        .withColumn(\"primaryTitle_lower\", F.lower(\"primaryTitle\")) \\\n",
    "        .withColumn(\"soup\",F.concat_ws(' ','directors','genres'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tconst</th>\n",
       "      <th>primaryTitle</th>\n",
       "      <th>startYear</th>\n",
       "      <th>genres</th>\n",
       "      <th>directors</th>\n",
       "      <th>averageRating</th>\n",
       "      <th>numVotes</th>\n",
       "      <th>primaryTitle_lower</th>\n",
       "      <th>soup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tt0000008</td>\n",
       "      <td>Edison Kinetoscopic Record of a Sneeze</td>\n",
       "      <td>1894.0</td>\n",
       "      <td>documentary short</td>\n",
       "      <td>nm0005690</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2068</td>\n",
       "      <td>edison kinetoscopic record of a sneeze</td>\n",
       "      <td>nm0005690 documentary short</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tt0000015</td>\n",
       "      <td>Autour d'une cabine</td>\n",
       "      <td>1894.0</td>\n",
       "      <td>animation short</td>\n",
       "      <td>nm0721526</td>\n",
       "      <td>6.2</td>\n",
       "      <td>1033</td>\n",
       "      <td>autour d'une cabine</td>\n",
       "      <td>nm0721526 animation short</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tt0000242</td>\n",
       "      <td>The Sign of the Cross</td>\n",
       "      <td>1899.0</td>\n",
       "      <td>fantasy horror short</td>\n",
       "      <td>nm0617588</td>\n",
       "      <td>6.4</td>\n",
       "      <td>1174</td>\n",
       "      <td>the sign of the cross</td>\n",
       "      <td>nm0617588 fantasy horror short</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tt0000614</td>\n",
       "      <td>The Red Spectre</td>\n",
       "      <td>1907.0</td>\n",
       "      <td>fantasy horror short</td>\n",
       "      <td>nm0159015,nm0954087</td>\n",
       "      <td>6.6</td>\n",
       "      <td>1068</td>\n",
       "      <td>the red spectre</td>\n",
       "      <td>nm0159015,nm0954087 fantasy horror short</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tt0001223</td>\n",
       "      <td>Frankenstein</td>\n",
       "      <td>1910.0</td>\n",
       "      <td>fantasy horror sci-fi</td>\n",
       "      <td>nm0205986</td>\n",
       "      <td>6.4</td>\n",
       "      <td>4302</td>\n",
       "      <td>frankenstein</td>\n",
       "      <td>nm0205986 fantasy horror sci-fi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tt0001740</td>\n",
       "      <td>The Lonedale Operator</td>\n",
       "      <td>1911.0</td>\n",
       "      <td>drama romance short</td>\n",
       "      <td>nm0000428</td>\n",
       "      <td>6.5</td>\n",
       "      <td>1228</td>\n",
       "      <td>the lonedale operator</td>\n",
       "      <td>nm0000428 drama romance short</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      tconst                            primaryTitle  startYear  \\\n",
       "0  tt0000008  Edison Kinetoscopic Record of a Sneeze     1894.0   \n",
       "1  tt0000015                     Autour d'une cabine     1894.0   \n",
       "2  tt0000242                   The Sign of the Cross     1899.0   \n",
       "3  tt0000614                         The Red Spectre     1907.0   \n",
       "4  tt0001223                            Frankenstein     1910.0   \n",
       "5  tt0001740                   The Lonedale Operator     1911.0   \n",
       "\n",
       "                  genres            directors  averageRating  numVotes  \\\n",
       "0      documentary short            nm0005690            5.4      2068   \n",
       "1        animation short            nm0721526            6.2      1033   \n",
       "2   fantasy horror short            nm0617588            6.4      1174   \n",
       "3   fantasy horror short  nm0159015,nm0954087            6.6      1068   \n",
       "4  fantasy horror sci-fi            nm0205986            6.4      4302   \n",
       "5    drama romance short            nm0000428            6.5      1228   \n",
       "\n",
       "                       primaryTitle_lower  \\\n",
       "0  edison kinetoscopic record of a sneeze   \n",
       "1                     autour d'une cabine   \n",
       "2                   the sign of the cross   \n",
       "3                         the red spectre   \n",
       "4                            frankenstein   \n",
       "5                   the lonedale operator   \n",
       "\n",
       "                                       soup  \n",
       "0               nm0005690 documentary short  \n",
       "1                 nm0721526 animation short  \n",
       "2            nm0617588 fantasy horror short  \n",
       "3  nm0159015,nm0954087 fantasy horror short  \n",
       "4           nm0205986 fantasy horror sci-fi  \n",
       "5             nm0000428 drama romance short  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a pandas dataframe\n",
    "df.toPandas().head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a pipeline to process the data and create vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import CountVectorizer,StopWordsRemover, Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/juliendesmedt/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "stopwordList = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# define the tokenizer\n",
    "TO = Tokenizer(inputCol=\"soup\", outputCol=\"soup_tok\")\n",
    "# define the stop word remover\n",
    "SWR = StopWordsRemover(inputCol='soup_tok', outputCol='soup_cleaned', stopWords=stopwordList)\n",
    "# define bow model\n",
    "BOW = CountVectorizer(inputCol = 'soup_cleaned', outputCol = 'soup_vec')\n",
    "\n",
    "# inspect the output of one of the feature extractors\n",
    "temp_pipeline = Pipeline().setStages([TO, SWR, BOW]).fit(df)\n",
    "df = temp_pipeline.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id \n",
    "\n",
    "df = df.withColumn(\"ID\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/01 15:02:29 WARN DAGScheduler: Broadcasting large task binary with size 1039.7 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+---------+--------------------+-------------------+-------------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+---+\n",
      "|   tconst|        primaryTitle|startYear|              genres|          directors|averageRating|numVotes|  primaryTitle_lower|                soup|            soup_tok|        soup_cleaned|            soup_vec| ID|\n",
      "+---------+--------------------+---------+--------------------+-------------------+-------------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+---+\n",
      "|tt0000008|Edison Kinetoscop...|     1894|   documentary short|          nm0005690|          5.4|    2068|edison kinetoscop...|nm0005690 documen...|[nm0005690, docum...|[nm0005690, docum...|(28643,[13,20,179...|  0|\n",
      "|tt0000015| Autour d'une cabine|     1894|     animation short|          nm0721526|          6.2|    1033| autour d'une cabine|nm0721526 animati...|[nm0721526, anima...|[nm0721526, anima...|(28643,[6,20,7677...|  1|\n",
      "|tt0000242|The Sign of the C...|     1899|fantasy horror short|          nm0617588|          6.4|    1174|the sign of the c...|nm0617588 fantasy...|[nm0617588, fanta...|[nm0617588, fanta...|(28643,[8,10,20,1...|  2|\n",
      "|tt0000614|     The Red Spectre|     1907|fantasy horror short|nm0159015,nm0954087|          6.6|    1068|     the red spectre|nm0159015,nm09540...|[nm0159015,nm0954...|[nm0159015,nm0954...|(28643,[8,10,20,1...|  3|\n",
      "|tt0001223|        Frankenstein|     1910|fantasy horror sc...|          nm0205986|          6.4|    4302|        frankenstein|nm0205986 fantasy...|[nm0205986, fanta...|[nm0205986, fanta...|(28643,[8,10,11,1...|  4|\n",
      "|tt0001740|The Lonedale Oper...|     1911| drama romance short|          nm0000428|          6.5|    1228|the lonedale oper...|nm0000428 drama r...|[nm0000428, drama...|[nm0000428, drama...|(28643,[0,5,20,74...|  5|\n",
      "|tt0002844|Fantômas: In the ...|     1913|         crime drama|          nm0275421|          6.9|    2328|fantômas: in the ...|nm0275421 crime d...|[nm0275421, crime...|[nm0275421, crime...|(28643,[0,3,2275]...|  6|\n",
      "|tt0003037|Fantomas: The Man...|     1913|         crime drama|          nm0275421|          6.9|    1584|fantomas: the man...|nm0275421 crime d...|[nm0275421, crime...|[nm0275421, crime...|(28643,[0,3,2275]...|  7|\n",
      "|tt0003733|          A Busy Day|     1914|        comedy short|          nm0784407|          4.8|    1165|          a busy day|nm0784407 comedy ...|[nm0784407, comed...|[nm0784407, comed...|(28643,[1,20,3644...|  8|\n",
      "|tt0003760|  Caught in the Rain|     1914|        comedy short|          nm0000122|          5.6|    1021|  caught in the rain|nm0000122 comedy ...|[nm0000122, comed...|[nm0000122, comed...|(28643,[1,20,58],...|  9|\n",
      "+---------+--------------------+---------+--------------------+-------------------+-------------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17179871794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25769804075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25769812703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34359738916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>42949674059</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID\n",
       "0  17179871794\n",
       "1  25769804075\n",
       "2  25769812703\n",
       "3  34359738916\n",
       "4  42949674059"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 174:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/01 14:05:45 ERROR RetryingBlockTransferor: Exception while beginning fetch of 1 outstanding blocks (after 3 retries)\n",
      "java.io.IOException: Connecting to /10.40.10.169:45911 timed out (120000 ms)\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:285)\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:126)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:154)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.lambda$initiateRetry$0(RetryingBlockTransferor.java:184)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/12/01 14:05:45 WARN BlockManager: Failed to fetch remote block taskresult_390 from [BlockManagerId(driver, 10.40.10.169, 45911, None)] after 1 fetch failures. Most recent failure cause:\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n",
      "\tat org.apache.spark.network.BlockTransferService.fetchBlockSync(BlockTransferService.scala:103)\n",
      "\tat org.apache.spark.storage.BlockManager.fetchRemoteManagedBuffer(BlockManager.scala:1154)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$getRemoteBlock$8(BlockManager.scala:1098)\n",
      "\tat scala.Option.orElse(Option.scala:447)\n",
      "\tat org.apache.spark.storage.BlockManager.getRemoteBlock(BlockManager.scala:1098)\n",
      "\tat org.apache.spark.storage.BlockManager.getRemoteBytes(BlockManager.scala:1238)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter$$anon$3.$anonfun$run$1(TaskResultGetter.scala:88)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:63)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.IOException: Connecting to /10.40.10.169:45911 timed out (120000 ms)\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:285)\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:126)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:154)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.lambda$initiateRetry$0(RetryingBlockTransferor.java:184)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\t... 1 more\n",
      "22/12/01 14:05:45 WARN TaskSetManager: Lost task 0.0 in stage 174.0 (TID 390) (10.40.10.169 executor driver): TaskResultLost (result lost from block manager)\n",
      "22/12/01 14:05:45 ERROR TaskSetManager: Task 0 in stage 174.0 failed 1 times; aborting job\n"
     ]
    }
   ],
   "source": [
    "# filter on the spark data set data with specific requirements\n",
    "df.filter(df.primaryTitle=='Titanic').select(\"ID\").toPandas().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I tried to reproduced the cosin function but without success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg.distributed import IndexedRowMatrix\n",
    "mat = IndexedRowMatrix(df).toBlockMatrix()\n",
    "dot = mat.multiply(mat.transpose())\n",
    "dot.toLocalMatrix().toArray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1 with sql commands (main issue is that doing it on all vector is too big)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/01 15:08:30 WARN DAGScheduler: Broadcasting large task binary with size 1070.1 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.               (0 + 4) / 100]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juliendesmedt/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/juliendesmedt/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/lib/python3.8/socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m dot_udf \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mudf(\u001b[39mlambda\u001b[39;00m x,y: \u001b[39mfloat\u001b[39m(x\u001b[39m.\u001b[39mdot(y)), DoubleType())\n\u001b[0;32m----> 2\u001b[0m df\u001b[39m.\u001b[39malias(\u001b[39m\"\u001b[39m\u001b[39mi\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mjoin(df\u001b[39m.\u001b[39malias(\u001b[39m\"\u001b[39m\u001b[39mj\u001b[39m\u001b[39m\"\u001b[39m), F\u001b[39m.\u001b[39mcol(\u001b[39m\"\u001b[39m\u001b[39mi.ID\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m<\u001b[39m F\u001b[39m.\u001b[39mcol(\u001b[39m\"\u001b[39m\u001b[39mj.ID\u001b[39m\u001b[39m\"\u001b[39m))\\\n\u001b[1;32m      3\u001b[0m     \u001b[39m.\u001b[39mselect(\n\u001b[1;32m      4\u001b[0m         F\u001b[39m.\u001b[39mcol(\u001b[39m\"\u001b[39m\u001b[39mi.ID\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39malias(\u001b[39m\"\u001b[39m\u001b[39mi\u001b[39m\u001b[39m\"\u001b[39m), \n\u001b[1;32m      5\u001b[0m         F\u001b[39m.\u001b[39mcol(\u001b[39m\"\u001b[39m\u001b[39mj.ID\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39malias(\u001b[39m\"\u001b[39m\u001b[39mj\u001b[39m\u001b[39m\"\u001b[39m), \n\u001b[1;32m      6\u001b[0m         dot_udf(\u001b[39m\"\u001b[39m\u001b[39mi.soup_vec\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mj.soup_vec\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39malias(\u001b[39m\"\u001b[39m\u001b[39mdot\u001b[39m\u001b[39m\"\u001b[39m))\\\n\u001b[1;32m      7\u001b[0m     \u001b[39m.\u001b[39msort(\u001b[39m\"\u001b[39m\u001b[39mi\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mj\u001b[39m\u001b[39m\"\u001b[39m)\\\n\u001b[1;32m      8\u001b[0m     \u001b[39m.\u001b[39mlimit(\u001b[39m10\u001b[39m) \\\n\u001b[1;32m      9\u001b[0m     \u001b[39m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/spark/python/pyspark/sql/dataframe.py:606\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    603\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mParameter \u001b[39m\u001b[39m'\u001b[39m\u001b[39mvertical\u001b[39m\u001b[39m'\u001b[39m\u001b[39m must be a bool\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    605\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(truncate, \u001b[39mbool\u001b[39m) \u001b[39mand\u001b[39;00m truncate:\n\u001b[0;32m--> 606\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mshowString(n, \u001b[39m20\u001b[39;49m, vertical))\n\u001b[1;32m    607\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    608\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1320\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1313\u001b[0m args_command, temp_args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_args(\u001b[39m*\u001b[39margs)\n\u001b[1;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client\u001b[39m.\u001b[39;49msend_command(command)\n\u001b[1;32m   1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_id, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49msend_command(command)\n\u001b[1;32m   1039\u001b[0m     \u001b[39mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[39mreturn\u001b[39;00m response, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[39m=\u001b[39m smart_decode(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstream\u001b[39m.\u001b[39;49mreadline()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mAnswer received: \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[39m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[39m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.8/socket.py:669\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    668\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 669\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    670\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    671\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dot_udf = F.udf(lambda x,y: float(x.dot(y)), DoubleType())\n",
    "df.alias(\"i\").join(df.alias(\"j\"), F.col(\"i.ID\") < F.col(\"j.ID\"))\\\n",
    "    .select(\n",
    "        F.col(\"i.ID\").alias(\"i\"), \n",
    "        F.col(\"j.ID\").alias(\"j\"), \n",
    "        dot_udf(\"i.soup_vec\", \"j.soup_vec\").alias(\"dot\"))\\\n",
    "    .sort(\"i\", \"j\")\\\n",
    "    .limit(10) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2 DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 208:==========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/01 14:37:22 WARN DAGScheduler: Broadcasting large task binary with size 1044.2 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/01 14:37:29 WARN DAGScheduler: Broadcasting large task binary with size 1055.2 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.                 (0 + 4) / 5]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juliendesmedt/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/juliendesmedt/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/lib/python3.8/socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [69], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmllib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlinalg\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistributed\u001b[39;00m \u001b[39mimport\u001b[39;00m IndexedRow, IndexedRowMatrix\n\u001b[0;32m----> 3\u001b[0m mat \u001b[39m=\u001b[39m IndexedRowMatrix(\n\u001b[1;32m      4\u001b[0m     df\u001b[39m.\u001b[39mselect(\u001b[39m\"\u001b[39m\u001b[39mID\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39msoup_vec\u001b[39m\u001b[39m\"\u001b[39m)\\\n\u001b[1;32m      5\u001b[0m         \u001b[39m.\u001b[39mrdd\u001b[39m.\u001b[39mmap(\u001b[39mlambda\u001b[39;00m row: IndexedRow(row\u001b[39m.\u001b[39mID, row\u001b[39m.\u001b[39msoup_vec\u001b[39m.\u001b[39mtoArray())))\u001b[39m.\u001b[39mtoBlockMatrix()\n",
      "File \u001b[0;32m~/spark/python/pyspark/mllib/linalg/distributed.py:805\u001b[0m, in \u001b[0;36mIndexedRowMatrix.toBlockMatrix\u001b[0;34m(self, rowsPerBlock, colsPerBlock)\u001b[0m\n\u001b[1;32m    775\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtoBlockMatrix\u001b[39m(\u001b[39mself\u001b[39m, rowsPerBlock: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m1024\u001b[39m, colsPerBlock: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m1024\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mBlockMatrix\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    776\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    777\u001b[0m \u001b[39m    Convert this matrix to a BlockMatrix.\u001b[39;00m\n\u001b[1;32m    778\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    803\u001b[0m \u001b[39m    3\u001b[39;00m\n\u001b[1;32m    804\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 805\u001b[0m     java_block_matrix \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_java_matrix_wrapper\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m    806\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mtoBlockMatrix\u001b[39;49m\u001b[39m\"\u001b[39;49m, rowsPerBlock, colsPerBlock\n\u001b[1;32m    807\u001b[0m     )\n\u001b[1;32m    808\u001b[0m     \u001b[39mreturn\u001b[39;00m BlockMatrix(java_block_matrix, rowsPerBlock, colsPerBlock)\n",
      "File \u001b[0;32m~/spark/python/pyspark/mllib/common.py:157\u001b[0m, in \u001b[0;36mJavaModelWrapper.call\u001b[0;34m(self, name, *a)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall\u001b[39m(\u001b[39mself\u001b[39m, name: \u001b[39mstr\u001b[39m, \u001b[39m*\u001b[39ma: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    156\u001b[0m     \u001b[39m\"\"\"Call method of java_model\"\"\"\u001b[39;00m\n\u001b[0;32m--> 157\u001b[0m     \u001b[39mreturn\u001b[39;00m callJavaFunc(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sc, \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_java_model, name), \u001b[39m*\u001b[39;49ma)\n",
      "File \u001b[0;32m~/spark/python/pyspark/mllib/common.py:131\u001b[0m, in \u001b[0;36mcallJavaFunc\u001b[0;34m(sc, func, *args)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[39m\"\"\"Call Java Function\"\"\"\u001b[39;00m\n\u001b[1;32m    130\u001b[0m java_args \u001b[39m=\u001b[39m [_py2java(sc, a) \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m args]\n\u001b[0;32m--> 131\u001b[0m \u001b[39mreturn\u001b[39;00m _java2py(sc, func(\u001b[39m*\u001b[39;49mjava_args))\n",
      "File \u001b[0;32m~/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1320\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1313\u001b[0m args_command, temp_args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_args(\u001b[39m*\u001b[39margs)\n\u001b[1;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client\u001b[39m.\u001b[39;49msend_command(command)\n\u001b[1;32m   1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_id, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49msend_command(command)\n\u001b[1;32m   1039\u001b[0m     \u001b[39mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[39mreturn\u001b[39;00m response, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[39m=\u001b[39m smart_decode(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstream\u001b[39m.\u001b[39;49mreadline()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mAnswer received: \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[39m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[39m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.8/socket.py:669\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    668\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 669\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    670\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    671\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.linalg.distributed import IndexedRow, IndexedRowMatrix\n",
    "\n",
    "movies_names = []\n",
    "\n",
    "mat = IndexedRowMatrix(\n",
    "    df.select(\"ID\", \"soup_vec\")\\\n",
    "        .rdd.map(lambda row: IndexedRow(row.ID, row.soup_vec.toArray()))).toBlockMatrix()\n",
    "\n",
    "for i in movies_names:\n",
    "    dot = mat[i].multiply(mat.transpose())\n",
    "    dot.toLocalMatrix().toArray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next of non pyspark code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the Cosine Similarity matrix based on the count_matrix\n",
    "cosine_sim2 = cosine_similarity(count_matrix, count_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cosine_sim2.shape)\n",
    "print(cosine_sim2.dtypes)\n",
    "\n",
    "print(cosine_sim2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 14891)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function that takes in movie title as input and outputs most similar movies\n",
    "def get_recommendations(title, cosine_sim=cosine_sim2):\n",
    "    # Get the index of the movie that matches the title\n",
    "    idx = df_clean.index[df_clean['primaryTitle'] == title]\n",
    "    print(f\"1. idx : {idx}\")\n",
    "    print(idx)\n",
    "\n",
    "    # Get the pairwsie similarity scores of all movies with that movie\n",
    "    sim_scores = cosine_sim2[idx]\n",
    "    print(f\"2. sim_scores : {sim_scores}\")\n",
    "    print(sim_scores.shape)\n",
    "    df_sim=pd.DataFrame(sim_scores.reshape(-1),columns=['cos'])\n",
    "\n",
    "    df_sim.reset_index()\n",
    "\n",
    "    df_sim.columns\n",
    "    # print(1000000000000000000)\n",
    "    # print(df_sim.head())\n",
    "\n",
    "    rslt_df = df_sim.sort_values(by='cos',ascending=False)\n",
    "\n",
    "    # print(rslt_df[0:11])\n",
    "    rec_list=rslt_df.reset_index()\n",
    "    list1=[]\n",
    "    list1=rec_list['index'][1:11].values\n",
    "\n",
    "    # Return the top 10 most similar movies\n",
    "    return df_clean['primaryTitle'].iloc[list1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that takes in movie title as input and outputs most similar movies\n",
    "def get_recommendations_list(liked_movies_list, cosine_sim=cosine_sim2):\n",
    "    # Get the index of the movie that matches the title\n",
    "    \n",
    "    idx_list=[]\n",
    "    for m in liked_movies_list:\n",
    "        \n",
    "        idx = df_clean.index[df_clean['primaryTitle'] == m]\n",
    "        idx_list.append(idx)\n",
    "\n",
    "    df_sim_list=pd.DataFrame()\n",
    "    print(idx_list)\n",
    "    for m in idx_list:\n",
    "        sim_scores = cosine_sim2[m]\n",
    "        df_sim=pd.DataFrame(sim_scores.reshape(-1),columns=[m])\n",
    "        df_sim_list[m]=df_sim\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    df_sim_list['average']=df_sim_list.mean(numeric_only=True, axis=1)\n",
    "\n",
    "    rslt_df = df_sim_list.sort_values(by='average',ascending=False)\n",
    "\n",
    "    # print(rslt_df[0:11])\n",
    "    rec_list=rslt_df.reset_index()\n",
    "    list1=[]\n",
    "    list1=rec_list['index'][0:11].values\n",
    "\n",
    "    # Return the top 10 most similar movies\n",
    "    return df_clean['primaryTitle'].iloc[list1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. idx : Int64Index([3], dtype='int64')\n",
      "2. sim_scores : [(0, array([0.25     , 0.       , 0.25     , ..., 0.2236068, 0.       ,\n",
      "       0.       ]))]\n",
      "3. sorted sim_scores : [(0, array([0.25     , 0.       , 0.25     , ..., 0.2236068, 0.       ,\n",
      "       0.       ]))]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'iloc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [60], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m var \u001b[39m=\u001b[39m get_recommendations(\u001b[39m'\u001b[39;49m\u001b[39mThe Kid\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(var)\n",
      "Cell \u001b[0;32mIn [59], line 16\u001b[0m, in \u001b[0;36mget_recommendations\u001b[0;34m(title, cosine_sim)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m3. sorted sim_scores : \u001b[39m\u001b[39m{\u001b[39;00msim_scores\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[39m# Get the scores of the 10 most similar movies\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m sim_scores \u001b[39m=\u001b[39m sim_scores\u001b[39m.\u001b[39;49miloc[\u001b[39m1\u001b[39m:\u001b[39m11\u001b[39m]\n\u001b[1;32m     17\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m4. top 10 sim_scores : \u001b[39m\u001b[39m{\u001b[39;00msim_scores\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[39m# Get the movie indices\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'iloc'"
     ]
    }
   ],
   "source": [
    "var = get_recommendations('The Kid')\n",
    "print(var)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
